{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(1, 7, 512)\n",
      "Wq.shape=(512, 512)\n",
      "Wk.shape=(512, 512)\n",
      "Wv.shape=(512, 512)\n"
     ]
    }
   ],
   "source": [
    "n_seq = 7\n",
    "x = tf.constant(np.random.normal(size=(1,n_seq,512)), dtype='float32')\n",
    "Wq = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wk = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wv = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "\n",
    "print('x.shape={}'.format(x.shape))\n",
    "print('Wq.shape={}'.format(Wq.shape))\n",
    "print('Wk.shape={}'.format(Wk.shape))\n",
    "print('Wv.shape={}'.format(Wv.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "\"i kicked the ball and it disappeared\"\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, x):\n",
    "        q = tf.matmul(x,self.Wq)\n",
    "        k = tf.matmul(x,self.Wk)\n",
    "        v = tf.matmul(x,self.Wv)\n",
    "\n",
    "        p = tf.nn.softmax(tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d))\n",
    "        p = tf.squeeze(p)\n",
    "\n",
    "        h = tf.matmul(p, v)\n",
    "        return h,p\n",
    "\n",
    "layer = SelfAttentionLayer(512)\n",
    "h, p = layer(x)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "multi_attn_head = [SelfAttentionLayer(64) for i in range(8)]\n",
    "outputs = [head(x)[0] for head in multi_attn_head]\n",
    "\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "class FCLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d1, d2):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d1), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            shape=(self.d1,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.W2 = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d2), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b2 = self.add_weight(\n",
    "            shape=(self.d2,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )  \n",
    "    \n",
    "    def call(self, x):\n",
    "        ff1 = tf.nn.relu(tf.matmul(x,self.W1)+self.b1)\n",
    "        ff2 = tf.matmul(x,self.W2)+self.b2\n",
    "        return ff2\n",
    "    \n",
    "ff = FCLayer(2048, 512)(h)\n",
    "print(ff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "\"i kicked the ball and it disappeared\"\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        q = tf.matmul(x,self.Wq)\n",
    "        k = tf.matmul(x,self.Wk)\n",
    "        v = tf.matmul(x,self.Wv)\n",
    "\n",
    "        p = tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d)\n",
    "        p = tf.squeeze(p)\n",
    "        if mask is None:\n",
    "            p = tf.nn.softmax(p)\n",
    "        else:\n",
    "            p += mask * -1e9\n",
    "            p = tf.nn.softmax(p)\n",
    "                \n",
    "\n",
    "        h = tf.matmul(p, v)\n",
    "        return h,p\n",
    "\n",
    "layer = SelfAttentionLayer(512)\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "h, p = layer(x, mask)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]], shape=(7, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.888 0.112 0.    0.    0.    0.    0.   ]\n",
      " [0.051 0.028 0.921 0.    0.    0.    0.   ]\n",
      " [0.261 0.556 0.142 0.042 0.    0.    0.   ]\n",
      " [0.592 0.191 0.068 0.068 0.081 0.    0.   ]\n",
      " [0.141 0.275 0.106 0.17  0.183 0.126 0.   ]\n",
      " [0.482 0.11  0.117 0.073 0.063 0.134 0.022]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "print(p.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n",
      "tf.Tensor([1.5 1.5], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.,2.])\n",
    "b = tf.constant([2.,1.])\n",
    "print(tf.stack([a,b])[0,:])\n",
    "print(tf.reduce_mean(tf.stack([a,b]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "multi_attn_head = [SelfAttentionLayer(512) for i in range(8)]\n",
    "outputs = [head(x)[0] for head in multi_attn_head]\n",
    "outputs = tf.math.add_n(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
