{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inputs and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(1, 7, 512)\n",
      "Wq.shape=(512, 512)\n",
      "Wk.shape=(512, 512)\n",
      "Wv.shape=(512, 512)\n"
     ]
    }
   ],
   "source": [
    "n_seq = 7\n",
    "x = tf.constant(np.random.normal(size=(1,n_seq,512)), dtype='float32')\n",
    "Wq = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wk = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wv = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "\n",
    "print('x.shape={}'.format(x.shape))\n",
    "print('Wq.shape={}'.format(Wq.shape))\n",
    "print('Wk.shape={}'.format(Wk.shape))\n",
    "print('Wv.shape={}'.format(Wv.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the SelfAttention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x):\n",
    "        q = tf.matmul(q_x,self.Wq)\n",
    "        k = tf.matmul(k_x,self.Wk)\n",
    "        v = tf.matmul(v_x,self.Wv)\n",
    "\n",
    "        p = tf.nn.softmax(tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d))\n",
    "        p = tf.squeeze(p)\n",
    "\n",
    "        h = tf.matmul(p, v)\n",
    "        return h,p\n",
    "\n",
    "layer = SelfAttentionLayer(512)\n",
    "h, p = layer(x, x, x)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Self-Attention in a Multi-head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "multi_attn_head = [SelfAttentionLayer(64) for i in range(8)]\n",
    "outputs = [head(x, x, x)[0] for head in multi_attn_head]\n",
    "\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "class FCLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d1, d2):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d1), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            shape=(self.d1,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.W2 = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d2), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b2 = self.add_weight(\n",
    "            shape=(self.d2,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )  \n",
    "    \n",
    "    def call(self, x):\n",
    "        ff1 = tf.nn.relu(tf.matmul(x,self.W1)+self.b1)\n",
    "        ff2 = tf.matmul(x,self.W2)+self.b2\n",
    "        return ff2\n",
    "    \n",
    "ff = FCLayer(2048, 512)(h)\n",
    "print(ff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a SelfAttention layer with Masking Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "\"i kicked the ball and it disappeared\"\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x, mask=None):\n",
    "        q = tf.matmul(q_x,self.Wq) #[None, t, d]\n",
    "        k = tf.matmul(k_x,self.Wk) #[None, t, d]\n",
    "        v = tf.matmul(v_x,self.Wv) #[None, t, d]\n",
    "        p = tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d) # [None, t, t]\n",
    "        \n",
    "        if mask is None:\n",
    "            p = tf.nn.softmax(p)\n",
    "        else:\n",
    "            p += mask * -1e9\n",
    "            p = tf.nn.softmax(p)\n",
    "                \n",
    "\n",
    "        h = tf.matmul(p, v) # [None, t, t] . [None, t, d] => [None, t, d]\n",
    "        return h,p\n",
    "\n",
    "layer = SelfAttentionLayer(512)\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "h, p = layer(x, x, x, mask)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Mask using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]], shape=(7, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Controlling decimal formatting\n",
    "#np.set_printoptions(precision=3)\n",
    "#print(p.numpy())\n",
    "\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "multi_attn_head = [SelfAttentionLayer(64) for i in range(8)]\n",
    "outputs = [head(x, x, x)[0] for head in multi_attn_head]\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder Layer and the Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d, n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d = d\n",
    "        self.d_head = int(d/n_heads) \n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(self.n_heads)]\n",
    "        self.fc_layer = FCLayer(2048, self.d)\n",
    " \n",
    "    def call(self, x):\n",
    "        print(x.shape)\n",
    "        def compute_multihead_output(x):\n",
    "            outputs = [head(x, x, x)[0] for head in self.attn_heads]            \n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "\n",
    "        h1 = compute_multihead_output(x)\n",
    "        y = self.fc_layer(h1)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d, n_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d = d\n",
    "        self.d_head = int(d/n_heads)\n",
    "        self.dec_attn_heads = [SelfAttentionLayer(self.d_head) for i in range(n_heads)]\n",
    "        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(n_heads)]\n",
    "        self.fc_layer = FCLayer(2048, self.d)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        [head.build(input_shape) for head in self.dec_attn_heads]\n",
    "        [head.build(input_shape) for head in self.attn_heads]\n",
    "        self.fc_layer.build(input_shape)\n",
    "        \n",
    "    def call(self, de_x, en_x, mask=None):\n",
    "        \n",
    "        def compute_multihead_output(de_x, en_x, mask=None):\n",
    "            outputs = [head(en_x, en_x, de_x, mask)[0] for head in self.attn_heads]\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "        \n",
    "        h1 = compute_multihead_output(de_x, de_x, mask)        \n",
    "        h2 = compute_multihead_output(h1, en_x)\n",
    "        y = self.fc_layer(h2)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Simple Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_22/Identity:0\", shape=(None, 25, 512), dtype=float32)\n",
      "(None, 25, 512)\n",
      "(None, 25, 512)\n",
      "Model: \"MinTransformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 25, 512)      153600      input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer_22 (EncoderLayer) (None, 25, 512)      2099712     embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 25, 512)      204800      input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer_23 (EncoderLayer) (None, 25, 512)      2099712     encoder_layer_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_11 (DecoderLayer) (None, 25, 512)      2886144     embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_12 (DecoderLayer) (None, 25, 512)      2886144     decoder_layer_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 25, 400)      205200      decoder_layer_12[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 10,535,312\n",
      "Trainable params: 10,535,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_steps = 25\n",
    "n_en_vocab = 300\n",
    "n_de_vocab = 400\n",
    "n_heads = 8\n",
    "d = 512\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((n_steps, n_steps)), -1, 0)\n",
    "\n",
    "en_inp = layers.Input(shape=(n_steps,))\n",
    "en_emb = layers.Embedding(n_en_vocab, 512, input_length=n_steps)(en_inp)\n",
    "print(en_emb)\n",
    "en_out1 = EncoderLayer(d, n_heads)(en_emb)\n",
    "en_out2 = EncoderLayer(d, n_heads)(en_out1)\n",
    "\n",
    "de_inp = layers.Input(shape=(n_steps,))\n",
    "de_emb = layers.Embedding(n_de_vocab, 512, input_length=n_steps)(de_inp)\n",
    "\n",
    "de_out1 = DecoderLayer(d, n_heads)(de_emb, en_out2, mask)\n",
    "de_out2 = DecoderLayer(d, n_heads)(de_out1, en_out2, mask)\n",
    "de_pred = layers.Dense(n_de_vocab, activation='softmax')(de_out2)\n",
    "\n",
    "transformer = models.Model(inputs=[en_inp, de_inp], outputs=de_pred, name='MinTransformer')\n",
    "transformer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
