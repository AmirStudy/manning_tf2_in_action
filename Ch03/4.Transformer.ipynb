{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers are the latest addition to the deep learning portfolio. Transformer networks started from the natural language processing domain and has permeated to other areas like computer vision. This was introduced in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et. al. In this chapter, we are going to look at various technical details of how to implement a Transformer ourselves. The Transformer has two main parts.\n",
    "\n",
    "* Encoder \n",
    "* Decoder\n",
    "\n",
    "Both the encoder and the decoder has multiple layers in it, and each layer consists of following types of sub layers.\n",
    "\n",
    "* Self-attention layer - This layer allows to pay attention to other inputs words in the sequence while processing a given input word. For example if the model is processing the word \"it\" from the sentence \"I kicked the ball and it disappeared\", this layer can pay attention the word \"ball\" while processing the word \"it\". This ability is quite important when it comes to natural language related tasks.\n",
    "* Fully connected layer - This adds depth to the model, allowing the model to learn more and more high level features.\n",
    "\n",
    "\n",
    "\n",
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inputs and variables\n",
    "\n",
    "Here we define an input sequence (having 7 timesteps) and the respective variables that will be in a self-attention layer. The input to the model is a batch of sentences, and each sentence is a sequence of words, and each word is represented as a vector (e.g. word embeddings). In summary, the input has following dimensions.\n",
    "\n",
    "* Batch dimension (1) - In our example, a batch will have a single sentence\n",
    "* Time dimension (7) - In our example, the sentence has 7 words\n",
    "* Feature dimension (512) - In our example, each word is represented by a 512 elements long word vector\n",
    "\n",
    "The weights are of size $n \\times n$, where $n$ is the feature dimensionality of the input to that layer. This means that during the computations, the weight matrix takes a $b \\times t \\times n$ input and prodicues a $b \\times t \\times n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(1, 7, 512)\n",
      "Wq.shape=(512, 512)\n",
      "Wk.shape=(512, 512)\n",
      "Wv.shape=(512, 512)\n"
     ]
    }
   ],
   "source": [
    "n_seq = 7\n",
    "x = tf.constant(np.random.normal(size=(1,n_seq,512)), dtype='float32')\n",
    "Wq = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wk = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "Wv = tf.Variable(np.random.normal(size=(512,512)), dtype='float32')\n",
    "\n",
    "print('x.shape={}'.format(x.shape))\n",
    "print('Wq.shape={}'.format(Wq.shape))\n",
    "print('Wk.shape={}'.format(Wk.shape))\n",
    "print('Wv.shape={}'.format(Wv.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the SelfAttention layer\n",
    "\n",
    "Here we are going to define the self-attention mechanism encapsulated in a Keras layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x):\n",
    "        q = tf.matmul(q_x,self.Wq)\n",
    "        k = tf.matmul(k_x,self.Wk)\n",
    "        v = tf.matmul(v_x,self.Wv)\n",
    "\n",
    "        p = tf.nn.softmax(tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d))\n",
    "        p = tf.squeeze(p)\n",
    "\n",
    "        h = tf.matmul(p, v)\n",
    "        return h,p\n",
    "\n",
    "layer = SelfAttentionLayer(512)\n",
    "h, p = layer(x, x, x)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Self-Attention in a Multi-head Attention Layer\n",
    "\n",
    "In the Transformer network found in the original paper, the self-attention layers are found as multi-head attention layers. This means each layer has multiple smaller attention layers. This enables the model to learn more robust features than when having a single big attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "multi_attn_head = [SelfAttentionLayer(64) for i in range(8)]\n",
    "outputs = [head(x, x, x)[0] for head in multi_attn_head]\n",
    "\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fully-Connected Layer\n",
    "\n",
    "Now let's define the fully-connected sublayer found in the Transformer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "class FCLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d1, d2):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.d1 = d1\n",
    "        self.d2 = d2\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W1 = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d1), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b1 = self.add_weight(\n",
    "            shape=(self.d1,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.W2 = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d2), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.b2 = self.add_weight(\n",
    "            shape=(self.d2,), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )  \n",
    "    \n",
    "    def call(self, x):\n",
    "        ff1 = tf.nn.relu(tf.matmul(x,self.W1)+self.b1)\n",
    "        ff2 = tf.matmul(x,self.W2)+self.b2\n",
    "        return ff2\n",
    "    \n",
    "ff = FCLayer(2048, 512)(h)\n",
    "print(ff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a SelfAttention layer with Masking Capability\n",
    "\n",
    "The decoder model uses a special type of attention layer. This attention layer masks any words ahead of the current word being processed. This in turn helps to avoid the decoder from seeing words it shouldn't during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "\"i kicked the ball and it disappeared\"\n",
    "class SelfAttentionLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.d = d\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.Wq = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )        \n",
    "        self.Wk = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "        self.Wv = self.add_weight(\n",
    "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
    "            trainable=True, dtype='float32'\n",
    "        )\n",
    "    \n",
    "    def call(self, q_x, k_x, v_x, mask=None):\n",
    "        q = tf.matmul(q_x,self.Wq) #[None, t, d]\n",
    "        k = tf.matmul(k_x,self.Wk) #[None, t, d]\n",
    "        v = tf.matmul(v_x,self.Wv) #[None, t, d]\n",
    "        p = tf.matmul(q, k, transpose_b=True)/math.sqrt(self.d) # [None, t, t]\n",
    "        \n",
    "        if mask is None:\n",
    "            p = tf.nn.softmax(p)\n",
    "        else:\n",
    "            p += mask * -1e9\n",
    "            p = tf.nn.softmax(p)\n",
    "                \n",
    "\n",
    "        h = tf.matmul(p, v) # [None, t, t] . [None, t, d] => [None, t, d]\n",
    "        return h,p\n",
    "\n",
    "layer = SelfAttentionLayer(512)\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "h, p = layer(x, x, x, mask)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Mask using TensorFlow\n",
    "\n",
    "Let's now define an example mask in TensorFlow. As we have defined an input sequence having 7 steps, let's define a mask for 7 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]], shape=(7, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Controlling decimal formatting\n",
    "#np.set_printoptions(precision=3)\n",
    "#print(p.numpy())\n",
    "\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((7, 7)), -1, 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "multi_attn_head = [SelfAttentionLayer(64) for i in range(8)]\n",
    "outputs = [head(x, x, x)[0] for head in multi_attn_head]\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder Layer and the Decoder Layer\n",
    "\n",
    "With all the low-level details ironed out, let's define the encoder and decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d, n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d = d\n",
    "        self.d_head = int(d/n_heads) \n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(self.n_heads)]\n",
    "        self.fc_layer = FCLayer(2048, self.d)\n",
    " \n",
    "    def call(self, x):\n",
    "        print(x.shape)\n",
    "        def compute_multihead_output(x):\n",
    "            outputs = [head(x, x, x)[0] for head in self.attn_heads]            \n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "\n",
    "        h1 = compute_multihead_output(x)\n",
    "        y = self.fc_layer(h1)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, d, n_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d = d\n",
    "        self.d_head = int(d/n_heads)\n",
    "        self.dec_attn_heads = [SelfAttentionLayer(self.d_head) for i in range(n_heads)]\n",
    "        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(n_heads)]\n",
    "        self.fc_layer = FCLayer(2048, self.d)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        [head.build(input_shape) for head in self.dec_attn_heads]\n",
    "        [head.build(input_shape) for head in self.attn_heads]\n",
    "        self.fc_layer.build(input_shape)\n",
    "        \n",
    "    def call(self, de_x, en_x, mask=None):\n",
    "        \n",
    "        def compute_multihead_output(de_x, en_x, mask=None):\n",
    "            outputs = [head(en_x, en_x, de_x, mask)[0] for head in self.attn_heads]\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "            return outputs\n",
    "        \n",
    "        h1 = compute_multihead_output(de_x, de_x, mask)        \n",
    "        h2 = compute_multihead_output(h1, en_x)\n",
    "        y = self.fc_layer(h2)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Simple Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_22/Identity:0\", shape=(None, 25, 512), dtype=float32)\n",
      "(None, 25, 512)\n",
      "(None, 25, 512)\n",
      "Model: \"MinTransformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 25, 512)      153600      input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer_22 (EncoderLayer) (None, 25, 512)      2099712     embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 25, 512)      204800      input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer_23 (EncoderLayer) (None, 25, 512)      2099712     encoder_layer_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_11 (DecoderLayer) (None, 25, 512)      2886144     embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer_12 (DecoderLayer) (None, 25, 512)      2886144     decoder_layer_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 25, 400)      205200      decoder_layer_12[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 10,535,312\n",
      "Trainable params: 10,535,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_steps = 25\n",
    "n_en_vocab = 300\n",
    "n_de_vocab = 400\n",
    "n_heads = 8\n",
    "d = 512\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((n_steps, n_steps)), -1, 0)\n",
    "\n",
    "en_inp = layers.Input(shape=(n_steps,))\n",
    "en_emb = layers.Embedding(n_en_vocab, 512, input_length=n_steps)(en_inp)\n",
    "print(en_emb)\n",
    "en_out1 = EncoderLayer(d, n_heads)(en_emb)\n",
    "en_out2 = EncoderLayer(d, n_heads)(en_out1)\n",
    "\n",
    "de_inp = layers.Input(shape=(n_steps,))\n",
    "de_emb = layers.Embedding(n_de_vocab, 512, input_length=n_steps)(de_inp)\n",
    "\n",
    "de_out1 = DecoderLayer(d, n_heads)(de_emb, en_out2, mask)\n",
    "de_out2 = DecoderLayer(d, n_heads)(de_out1, en_out2, mask)\n",
    "de_pred = layers.Dense(n_de_vocab, activation='softmax')(de_out2)\n",
    "\n",
    "transformer = models.Model(inputs=[en_inp, de_inp], outputs=de_pred, name='MinTransformer')\n",
    "transformer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
